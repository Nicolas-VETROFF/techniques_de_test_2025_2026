# Retour d'expérience

Lors de ce TP, j’ai d’abord dû me familiariser avec l’environnement Python ainsi qu’avec plusieurs outils comme Flask, Pytest, Coverage et Pdoc3. La démarche adoptée reposait sur la méthode TDD (Test Driven Development), consistant à concevoir les tests avant même l’implémentation de l’application.

L’une des principales difficultés que j'ai pu rencontrer concernait la conception des tests sans disposer d’une vision précise de l’implémentation finale. Cet exercice s’est révélé compliqué pour moi, car il va à l’encontre de l’approche habituelle qui consiste à coder avant de réfléchir aux tests unitaires, approche que nous avons majoritairement adoptée depuis le début de notre scolarité en informatique (projets, TP, etc.), du moins pour ma part. Il était en effet difficile d’anticiper l’ensemble des comportements attendus sans avoir encore développé la logique métier et identifié précisément les erreurs à soulever, notemment pour l'API.

Par ailleurs, j’ai rencontré des difficultés dans la compréhension et l’utilisation du mécanisme de mock en Python. Son rôle exact et son intégration dans les tests n’étaient pas immédiatement clairs, même si j’avais déjà été confronté à des outils similaires en Java lors de mes années à l’IUT. Le passage à un nouvel écosystème et à de nouvelles bibliothèques m’a demandé un temps d’adaptation. De plus, la définition de seuils pertinents pour les tests de performance s’est avérée délicate, en particulier pour des ensembles de points de taille moyenne ou importante à trianguler. Les valeurs retenues pour 1 000 et 10 000 points ont donc été choisies de manière arbitraire, faute de références précises ou de contraintes formelles fournies.

Concernant le plan initial, celui-ci était globalement pertinent, notamment dans les différents tests à implémenter et sur une chaîne d’outils complète (tests, couverture, documentation). En revanche, avec le recul, ce plan manquait de description et ne couvrait, après coup, pas toutes les facettes du projet.

J'ai essayé au mieux de ne pas implémenter de fonctionnalités non couvertes par des tests. Malheureusement, j'ai constaté que certains cas d'erreur évidents n'avaient pas été pris en compte lors de l'écriture initiale des tests. Ces oublis ont été corrigés par la suite, une fois l'implémentation entamée, ce qui montre que la conception exhaustive des tests dès le départ reste un exercice difficile, en particulier lorsque le périmètre fonctionnel n'est pas encore totalement clair. Cela veut donc dire que j'ai rajouté des tests au cours de l'implémentation du corps de l'application.

J’ai également eu besoin de me familiariser avec les outils de test et de couverture, notamment Pytest et Coverage. L’utilisation de ces outils m’a également permis de mieux visualiser les zones du code insuffisamment testées et d’améliorer progressivement la qualité globale du projet. Je n'ai cependant pas réussi à atteindre un seuil de couverture au-dessus de 88%.

Si je devais refaire ce TP, je consacrerais davantage de temps en amont à la réflexion sur les cas limites et les scénarios d’erreur, quitte à faire évoluer les tests plus fréquemment.

Néanmoins, cette expérience m’a permis de constater les bénéfices concrets du TDD. Le fait de rédiger les tests avant toute implémentation s’est révélé très utile pour garantir le bon fonctionnement du code par la suite. Cette approche permet également de limiter les biais liés à l’implémentation préalable, qui conduit souvent à négliger les cas limites et les scénarios d’erreur. En TDD, l’objectif initial n’est pas uniquement de faire passer les tests, mais surtout de s’assurer qu’ils couvrent correctement les comportements attendus, ce qui contribue au final à améliorer la qualité globale du projet.